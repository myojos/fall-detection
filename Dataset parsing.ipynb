{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm, trange "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_videos_list_train_val_test(videos_list, train_ratio=0.7, val_ratio=0.2):\n",
    "    random.shuffle(videos_list)\n",
    "    train_index = int(np.floor(train_ratio * len(videos_list)))\n",
    "    val_index = train_index + int(np.floor(val_ratio * len(videos_list)))\n",
    "    return videos_list[:train_index], videos_list[train_index:val_index], videos_list[val_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 5\n",
    "SPATIAL_FEATURES = ['CTR_LOC_X', 'CTR_LOC_Y', 'CTR_SPD_X', 'CTR_SPD_Y', 'ANGLE', 'ANGLE_SPEED', 'WIDTH', 'HEIGHT', \n",
    "                    'ASPECT_RATIO', 'C_MOTION', 'PROJ_RATIO']\n",
    "\n",
    "# Constants\n",
    "CLOSING_KERNEL = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "EROTION_KERNEL = np.ones((2, 2), np.uint8)\n",
    "MIN_HUMAN_CONTOUR_AREA = 1000\n",
    "MHI_DURATION = 500 # milliseconds\n",
    "MHI_THRESHOLD = 32\n",
    "GAUSSIAN_KERNEL = (3, 3)\n",
    "\n",
    "# Kalman filter\n",
    "NO_CONTOUR_TOLERANCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path('data/URFD/')\n",
    "videos_folder = dataset_folder / 'videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the videos\n",
    "for fall in range(1, 31):\n",
    "    file_name = f\"fall-{fall:02}-cam0.mp4\"\n",
    "    link = f\"http://fenix.univ.rzeszow.pl/~mkepski/ds/data/\" + file_name\n",
    "    urllib.request.urlretrieve(link, str(videos_folder / file_name))\n",
    "    \n",
    "for adl in range(1, 41):\n",
    "    file_name = f\"adl-{adl:02}-cam0.mp4\"\n",
    "    link = f\"http://fenix.univ.rzeszow.pl/~mkepski/ds/data/\" + file_name\n",
    "    urllib.request.urlretrieve(link, str(videos_folder / file_name))\n",
    "\n",
    "urllib.request.urlretrieve('http://fenix.univ.rzeszow.pl/~mkepski/ds/data/urfall-cam0-falls.csv', \n",
    "                           str(dataset_folder / 'urfall-cam0-falls.csv'))\n",
    "\n",
    "# Create annotations dictionary\n",
    "df = pd.read_csv(dataset_folder / 'urfall-cam0-falls.csv', header=None, usecols=range(3))\n",
    "df.columns = ['file_name', 'frame', 'label']\n",
    "falls = df[df.label == 0].groupby('file_name', as_index=False).agg({'frame': ['min', 'max']})\n",
    "falls.columns = ['_'.join(col) if col[1] else col[0] for col in falls.columns.values]\n",
    "\n",
    "annotations = {}\n",
    "for row in falls.itertuples():\n",
    "    annotations[f'{row.file_name}-cam0.mp4'] = [row.frame_min, row.frame_max]\n",
    "\n",
    "# Save to file\n",
    "with open(dataset_folder / 'annotations.json', 'w') as f:\n",
    "    f.write(json.dumps(annotations, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_folder / 'annotations.json', 'r') as ano_file:\n",
    "    annotations = json.load(ano_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train / val / test split\n",
    "fall_videos = list(videos_folder.glob('fall*'))\n",
    "fall_train, fall_val, fall_test = split_videos_list_train_val_test(fall_videos)\n",
    "adl_videos = list(videos_folder.glob('adl*'))\n",
    "adl_train, adl_val, adl_test = split_videos_list_train_val_test(adl_videos)\n",
    "\n",
    "all_videos = {\n",
    "    \"train\": fall_train + adl_train,\n",
    "    \"val\": fall_val + adl_val,\n",
    "    \"test\": fall_test + adl_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": {\"spatial\": [], \"rgb\": [], \"flow\": []},\n",
    "    \"val\": {\"spatial\": [], \"rgb\": [], \"flow\": []},\n",
    "    \"test\": {\"spatial\": [], \"rgb\": [], \"flow\": []}\n",
    "}\n",
    "labels = {\"train\": [], \"val\": [], \"test\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd68e69a1ead49209a510c202cf61615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train videos', max=49.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='frame', max=99.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb00a4c95974ac2979cd9dd80a7eaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='frame', max=84.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-85d00a82d3d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mopt_current_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptical_flow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_last_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_current_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0moptical_flow_sequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "for group in all_videos:\n",
    "    for video in tqdm(all_videos[group], desc=f'{group} videos', total=len(all_videos[group])):\n",
    "    \n",
    "        cap = cv2.VideoCapture(str(video))\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # > Labels\n",
    "        fall_start, fall_end = annotations[video.name] if video.name.startswith('fall') else (-1, -1)  # Negative values for adl videos\n",
    "\n",
    "        # > Spatial features\n",
    "        spatial_features_vectors_sequence = []\n",
    "        background_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "        # ---> tMHI prep\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        ms_per_frame = 1000 / fps   # milliseconds\n",
    "        _, frame = cap.read()\n",
    "        frame = frame[:, 320:]\n",
    "        tmhi_last_frame = cv2.GaussianBlur(frame, GAUSSIAN_KERNEL, 0)\n",
    "        h, w = tmhi_last_frame.shape[:2]\n",
    "        mhi = np.zeros((h, w), np.float32)\n",
    "\n",
    "        # ---> Kalman Filter Prep\n",
    "        state_size = 8          # [x, y, v_x, v_y, alpha, v_alpha, a, b]\n",
    "        measurement_size = 5    # [x, y, alpha, a, b]\n",
    "        kalman = cv2.KalmanFilter(state_size, measurement_size, 0)\n",
    "        kalman.transitionMatrix = np.eye(state_size, dtype=np.float32)\n",
    "        kalman.transitionMatrix[(0,1,4), (2,3,5)] = ms_per_frame\n",
    "        kalman.measurementMatrix = np.zeros((measurement_size, state_size), dtype=np.float32)\n",
    "        pos = [(0,0), (1,1), (2,4), (3,6), (4,7)]\n",
    "        rows, cols = zip(*pos)\n",
    "        kalman.measurementMatrix[rows, cols] = 1.\n",
    "        kalman.processNoiseCov = 1e-5 * np.eye(state_size, dtype=np.float32)  # Values can't change sharply\n",
    "        kalman.processNoiseCov[4,4] = 0        # Angle shouldn't change very fast (unless there is a fall!)\n",
    "        kalman.processNoiseCov[(6,7),(6,7)] = 5e-4        # Height and width can change faster\n",
    "        kalman.measurementNoiseCov = np.eye(measurement_size, dtype=np.float32)\n",
    "        kalman.measurementNoiseCov[2,2] = 1e-1  # Angle is very noisy (especially around the limits)\n",
    "        kalman.measurementNoiseCov[(0,1),(0,1)] = 1  # Center is super noisy\n",
    "        kalman.measurementNoiseCov[(3,4),(3,4)] = 1e-1  # Height and width are not noisy\n",
    "        kalman_filter_active = False  # This will change according to our contour search\n",
    "        contour_unfound_count = 0     \n",
    "        last_angle = None  # Continous angle tracking (to avoid drastic changes on direction change)\n",
    "\n",
    "        # > Optical flow\n",
    "        optical_flow_sequence = []\n",
    "        optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        opt_last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        for frame_number in trange(2, length + 1, desc='frame', leave=False):\n",
    "            ret, frame = cap.read()\n",
    "            frame = frame[:, 320:]  # Remove the depth data\n",
    "\n",
    "            label = 0\n",
    "            if fall_start <= frame_number <= fall_end:\n",
    "                label = 1\n",
    "\n",
    "            ### OPTICAL FLOW\n",
    "            \n",
    "            opt_current_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            flow = optical_flow.calc(opt_last_frame, opt_current_frame, None)  \n",
    "            optical_flow_sequence.append(flow)\n",
    "                \n",
    "#             ### SPATIAL FEATURES\n",
    "\n",
    "#             # Set parameters to 0 unless found otherwise\n",
    "#             c_motion = 0\n",
    "#             projection_ratio = 0\n",
    "\n",
    "#             # Transformations to improve the background subtraction\n",
    "#             # tMHI\n",
    "#             img = cv2.GaussianBlur(frame, GAUSSIAN_KERNEL, 0)\n",
    "#             frame_diff = cv2.absdiff(img, tmhi_last_frame)\n",
    "#             gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "#             _, motion_mask = cv2.threshold(gray_diff, MHI_THRESHOLD, 1, cv2.THRESH_BINARY)\n",
    "#             cv2.motempl.updateMotionHistory(motion_mask, mhi, frame_number * ms_per_frame, MHI_DURATION)\n",
    "#             tmhi_last_frame = img\n",
    "\n",
    "#             # Preparation for contour finding\n",
    "#             img = cv2.GaussianBlur(frame, (5,5), 0)\n",
    "#             img = background_subtractor.apply(img)\n",
    "#             img = cv2.erode(img, EROTION_KERNEL, iterations=1)\n",
    "#             img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, CLOSING_KERNEL, iterations=5)\n",
    "#             img[img < 255] = 0\n",
    "\n",
    "#             # Find contours\n",
    "#             (contours, _) = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             if contours:\n",
    "#                 biggest_contour = max(contours, key=lambda c: cv2.contourArea(c))\n",
    "\n",
    "#                 # Check minimal requirements to fit new ellipse\n",
    "#                 if cv2.contourArea(biggest_contour) > MIN_HUMAN_CONTOUR_AREA and len(biggest_contour) >= 5:\n",
    "#                     # calculate motion coefficient\n",
    "#                     mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "#                     cv2.drawContours(mask, biggest_contour, -1, 1, -1)\n",
    "#                     tmhi = np.uint8(np.clip((mhi - (frame_number * ms_per_frame - MHI_DURATION)) / MHI_DURATION, 0, 1))\n",
    "#                     relevant_tmhi = np.multiply(mask, tmhi)\n",
    "#                     c_motion = np.sum(relevant_tmhi) / np.sum(mask) \n",
    "\n",
    "#                     # calculate projections\n",
    "#                     horizontal_proj = mask.any(axis=0).sum()\n",
    "#                     vertical_proj = mask.any(axis=1).sum()\n",
    "#                     projection_ratio = vertical_proj / horizontal_proj\n",
    "\n",
    "#                     contour_unfound_count = 0\n",
    "#                     measured_ellipse = cv2.fitEllipse(biggest_contour)\n",
    "#                     (x, y), (MA, ma), alpha = measured_ellipse\n",
    "#                     if not kalman_filter_active:\n",
    "#                         last_angle = alpha\n",
    "#                         kalman.errorCovPre = np.eye(state_size)\n",
    "#                         state = np.array([x, y, 0, 0, alpha, 0, MA, ma], dtype=np.float32)[:, np.newaxis]\n",
    "#                         kalman.statePost = state\n",
    "#                         kalman_filter_active = True\n",
    "#                     else:\n",
    "#                         corrected_alpha = min(alpha, alpha - 180, alpha + 180, key=lambda x: abs(x - last_angle))\n",
    "#                         kalman.correct(np.array([x, y, corrected_alpha, MA, ma], dtype=np.float32)[:, np.newaxis])\n",
    "#                         last_angle = corrected_alpha\n",
    "\n",
    "#                 else:\n",
    "#                     contour_unfound_count += 1\n",
    "#                     if contour_unfound_count >= NO_CONTOUR_TOLERANCE:\n",
    "#                         kalman_filter_active = False\n",
    "#                         last_angle = None\n",
    "\n",
    "#             else:\n",
    "#                 contour_unfound_count += 1\n",
    "#                 if contour_unfound_count >= NO_CONTOUR_TOLERANCE:\n",
    "#                     kalman_filter_active = False\n",
    "#                     last_angle = None\n",
    "\n",
    "#             if kalman_filter_active:\n",
    "#                 state = kalman.predict() \n",
    "#                 ctr_x, ctr_y, ctr_vx, ctr_vy, alpha, v_alpha, a, b = state.T[0]\n",
    "#                 spatial_features_vectors_sequence.append([ctr_x, ctr_y, ctr_vx, ctr_vy, alpha, v_alpha, a, b, \n",
    "#                                                           a / b, c_motion, projection_ratio])                    \n",
    "#             else:\n",
    "#                 spatial_features_vectors_sequence.append(np.zeros(len(SPATIAL_FEATURES)))\n",
    "\n",
    "            # when we have SEQ_LENGTH frame, we can start saving data\n",
    "            if frame_number > 1 + SEQ_LENGTH:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # convert to RGB\n",
    "#                 data[group][\"spatial\"].append(np.array(spatial_features_vectors_sequence))\n",
    "                data[group][\"rgb\"].append(rgb)\n",
    "                data[group][\"flow\"].append(np.array(optical_flow_sequence))\n",
    "                labels[group].append(label)\n",
    "#                 del spatial_features_vectors_sequence[0]\n",
    "                del optical_flow_sequence[0]\n",
    "\n",
    "\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the flow data\n",
    "all_flows = np.array(data['train']['flow'])\n",
    "max_value = all_flows.max()\n",
    "min_value = all_flows.min()\n",
    "for i in range(len(data['train']['flow'])):\n",
    "    data['train']['flow'][i] = 255 * data['train']['flow'][i] / (max_value - min_value)\n",
    "    data['train']['flow'][i] = data['train']['flow'][i].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(dataset_folder / \"data.hdf5\", \"w\") as f:\n",
    "    data_group = f.create_group(\"data\")\n",
    "    for split in data:\n",
    "        split_group = data_group.create_group(split)\n",
    "        for feature in data[split]:\n",
    "            split_group.create_dataset(feature, data=np.array(data[split][feature], dtype=np.uint8))\n",
    "\n",
    "    label_group = f.create_group(\"labels\")\n",
    "    for split in labels:\n",
    "        label_group.create_dataset(split, data=np.array(labels[split], dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SisFall ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path('data/SisFall/')\n",
    "videos = dataset / 'videos'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
