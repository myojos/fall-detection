{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run imports.py\n",
    "%matplotlib inline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP=\"exp2\"\n",
    "VIDEO=\"925\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'frames_per_clip': 10, 'step_between_clips': 10, 'batch_size': 32, 'transform': {'Normalize': {'mean': 0.449, 'std': 0.226}, 'HorizontalFlip': True, 'Cutout': False}, 'transformOF': {'Normalize': None, 'HorizontalFlip': True, 'Cutout': False}}, 'model': {'phase1': {'epochs': 10, 'lr': 0.005, 'step_size': 5, 'gamma': 0.5}, 'phase2': {'epochs': 45, 'lr': 2e-05, 'step_size': 15, 'gamma': 0.5}}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"exps/default_config.yaml\", \"r\") as f:\n",
    "    default_config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "exp_config = dict()\n",
    "if os.path.isfile(\"exps/\" + EXP + \"/config.yaml\"):\n",
    "    with open(\"exps/\" + EXP + \"/config.yaml\", \"r\") as f:\n",
    "        exp_config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "config = dict()\n",
    "config.update(default_config)\n",
    "config.update(exp_config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tansform_param = config[\"dataset\"][\"transform\"]\n",
    "\n",
    "tensor_aug = [transforms.ToTensor()]\n",
    "if tansform_param[\"Normalize\"]:\n",
    "    norm = tansform_param[\"Normalize\"]\n",
    "    tensor_aug.append(transforms.Normalize(norm[\"mean\"],norm[\"std\"]))\n",
    "    \n",
    "%run preprocessing/CustomTransform.py\n",
    "tansformOF_param = config[\"dataset\"][\"transformOF\"]\n",
    "\n",
    "tensor_augOF = [transforms.ToTensor()]\n",
    "if tansformOF_param[\"Normalize\"]:\n",
    "    norm = tansformOF_param[\"Normalize\"]\n",
    "    tensor_augOF.append(transforms.Normalize(norm[\"mean\"],norm[\"std\"]))\n",
    "    \n",
    "alb_rescale = alb.Resize(244, 244, always_apply=True)\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "  [AlbuWrapperNumpy(alb_rescale), *tensor_aug])\n",
    "\n",
    "\n",
    "test_transformsOF = transforms.Compose(\n",
    "  [AlbuWrapperNumpy(alb_rescale), *tensor_augOF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet\n",
    "# average input channels of conv layer and return new model\n",
    "def replace_conv(model, inplanes):\n",
    "    conv_weights = list(model.parameters())[0].clone().detach().numpy()\n",
    "    new_conv_weights = conv_weights.mean(axis=1)\n",
    "    new_conv_weights = np.repeat(new_conv_weights[:, np.newaxis, :, :], inplanes, axis=1)\n",
    "    new_conv = torch.nn.Conv2d(inplanes, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "    new_conv.weight = torch.nn.Parameter(torch.from_numpy(new_conv_weights))\n",
    "    model._modules['features'][0][0] = new_conv\n",
    "    return model\n",
    "\n",
    "def replace_classifier(model, outplanes):\n",
    "    classifier = torch.nn.Linear(in_features=1280, out_features=outplanes, bias=True)\n",
    "    model._modules['classifier'][1] = classifier\n",
    "    return model\n",
    "\n",
    "def get_model(freeze=True, inplanes=10, outplanes=2, pretrained=True,\n",
    "              start_model=None, model_name=\"model\"):\n",
    "    model = start_model\n",
    "    if start_model is None:\n",
    "        model = models.mobilenet_v2(pretrained=pretrained)\n",
    "\n",
    "    if inplanes != model._modules[\"features\"][0][0].in_channels:\n",
    "        model = replace_conv(model, inplanes)\n",
    "        \n",
    "    if outplanes != model._modules[\"classifier\"][1].out_features:\n",
    "        model = replace_classifier(model, outplanes)\n",
    "        \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = (not freeze)\n",
    "    input_batch = torch.ones((batch_size, inplanes, 32, 32))\n",
    "    writer = SummaryWriter(log_dir=f\"exps/{EXP}/logs/{model_name}\")\n",
    "    writer.add_graph(model, input_batch)\n",
    "    return model\n",
    "\n",
    "def toggle_freeze(model, freeze):\n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = (not freeze)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty 0.32879552245140076\n",
      "Empty 0.5249453186988831\n",
      "Empty 0.40442293882369995\n",
      "Empty 0.3521553575992584\n",
      "Empty 0.3650844395160675\n",
      "Empty 0.4276222288608551\n",
      "Empty 0.4924636483192444\n",
      "Empty 0.4962003827095032\n",
      "Empty 0.5867840647697449\n",
      "Standing 0.48491495847702026\n",
      "Standing 0.43587663769721985\n",
      "Standing 0.4332454204559326\n",
      "Standing 0.4376181662082672\n",
      "Standing 0.4845809042453766\n",
      "Sitting 0.5160655975341797\n",
      "Sitting 0.3991531729698181\n",
      "Sitting 0.4148784875869751\n",
      "Sitting 0.37118563055992126\n",
      "Sitting 0.41199666261672974\n",
      "Sitting 0.5983976721763611\n",
      "Sitting 0.36824700236320496\n",
      "Sitting 0.4902385473251343\n",
      "Standing 0.5197858810424805\n",
      "Standing 0.5240158438682556\n",
      "Standing 0.4002998173236847\n",
      "Standing 0.45862990617752075\n",
      "Standing 0.35471057891845703\n",
      "Empty 0.4100712537765503\n",
      "Empty 0.543353796005249\n",
      "Standing 0.4462784230709076\n",
      "Standing 0.6744210720062256\n",
      "Standing 0.39024409651756287\n",
      "Sitting 0.45892533659935\n",
      "Sitting 0.4997294843196869\n",
      "Lying 0.5590689182281494\n",
      "Lying 0.3891603648662567\n",
      "Lying 0.6464033126831055\n",
      "Lying 0.36704325675964355\n",
      "Lying 0.39025792479515076\n",
      "Lying 0.42654091119766235\n",
      "Lying 0.43220794200897217\n",
      "Lying 0.431277334690094\n",
      "Lying 0.5273481607437134\n",
      "Lying 0.6884313225746155\n",
      "Lying 0.467597097158432\n",
      "Lying 0.4533541798591614\n",
      "Lying 0.6776828169822693\n",
      "Lying 0.44796550273895264\n",
      "Lying 0.49907782673835754\n",
      "Lying 0.6001110672950745\n",
      "Lying 0.39664024114608765\n",
      "Lying 0.4862631857395172\n",
      "Sitting 0.4437093734741211\n",
      "Sitting 0.6282956004142761\n",
      "Bending 0.5324494242668152\n",
      "Standing 0.5239629745483398\n",
      "Standing 0.40941300988197327\n",
      "Standing 0.4491795003414154\n",
      "Standing 0.5244728326797485\n",
      "Standing 0.558424711227417\n",
      "Standing 0.6574301719665527\n",
      "Standing 0.4300105571746826\n",
      "Sitting 0.4412667155265808\n",
      "Sitting 0.7194920778274536\n",
      "Sitting 0.3425006866455078\n",
      "Sitting 0.5390519499778748\n",
      "Sitting 0.4184488356113434\n",
      "Sitting 0.5554035902023315\n",
      "Sitting 0.4610242247581482\n",
      "Sitting 0.516148567199707\n",
      "Sitting 0.560896098613739\n",
      "Standing 0.6020589470863342\n",
      "Standing 0.4024675190448761\n",
      "Standing 0.44920235872268677\n",
      "Standing 0.3259703516960144\n",
      "Standing 0.5567556619644165\n",
      "Standing 0.4529181122779846\n",
      "Sitting 0.5834461450576782\n",
      "Lying 0.37749767303466797\n",
      "Lying 0.4149349629878998\n",
      "Lying 0.6489918231964111\n",
      "Lying 0.42614656686782837\n",
      "Lying 0.43350815773010254\n",
      "Lying 0.3623580038547516\n",
      "Lying 0.5496975779533386\n",
      "Lying 0.5157383680343628\n",
      "Lying 0.40794098377227783\n",
      "Sitting 0.6044389605522156\n",
      "Sitting 0.47293591499328613\n",
      "Sitting 0.5111720561981201\n",
      "Standing 0.515004575252533\n",
      "Standing 0.5131444334983826\n"
     ]
    }
   ],
   "source": [
    "frames_per_clip = config[\"dataset\"][\"frames_per_clip\"]\n",
    "step_between_clips = config[\"dataset\"][\"step_between_clips\"]\n",
    "batch_size = config[\"dataset\"][\"batch_size\"]\n",
    "\n",
    "OUTPUT=\"project2.mp4\"\n",
    "\n",
    "PATH = f\"exps/{EXP}/\"\n",
    "MODEL_NAME=\"of_model\"\n",
    "MODEL_FILE=f\"{MODEL_NAME}_best.pth\"\n",
    "# MODEL_FILE=f\"{MODEL_NAME}_final.pth\"\n",
    "\n",
    "classes = [\"Empty\", \"Standing\", \"Sitting\", \"Lying\", \"Bending\", \"Crawling\"]\n",
    "model = get_model(inplanes=frames_per_clip*2, outplanes=6, model_name=MODEL_NAME)\n",
    "if os.path.isfile(os.path.join(PATH, MODEL_FILE)):\n",
    "    model.load_state_dict(torch.load(os.path.join(PATH, MODEL_FILE)))\n",
    "\n",
    "    \n",
    "    \n",
    "def put_text(img, label, prob):\n",
    "    font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (10,200)\n",
    "    fontScale              = 1\n",
    "    fontColor              = (255,255,0)\n",
    "    lineType               = 2\n",
    "\n",
    "    cv2.putText(img,f'{label} {round(prob * 100,2)}%', \n",
    "        bottomLeftCornerOfText, \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "    return img\n",
    "\n",
    "img_array = []\n",
    "BASE_RGB_PATH = f\"data/FD/{VIDEO}/rgb\"\n",
    "BASE_FLOW_PATH = f\"data/FD/{VIDEO}/flow\"\n",
    "BASE_CLIP_PATH = f\"data/FD/{VIDEO}/clip_{frames_per_clip}_{step_between_clips}\"\n",
    "out = cv2.VideoWriter(OUTPUT,cv2.VideoWriter_fourcc(*'MP4V'), 20, (320,240))\n",
    "label, conf = 0, 0\n",
    "for filename in os.listdir(BASE_RGB_PATH):\n",
    "    filepath = os.path.join(BASE_RGB_PATH, filename)\n",
    "    if not os.path.isfile(filepath):\n",
    "        continue\n",
    "    idx = int(filename[-8:-4])\n",
    "    img = cv2.imread(filepath)\n",
    "    height, width, layers = img.shape\n",
    "#     size = (width,height)\n",
    "    if idx % 10 == 1:\n",
    "        sample_file = os.path.join(BASE_CLIP_PATH, str(idx//10 * 10) + \".npz\")\n",
    "        if os.path.isfile(sample_file):\n",
    "            data = np.load(sample_file)[\"arr_0\"].astype(np.float32)\n",
    "            data = test_transformsOF.__call__(data)\n",
    "            data = data.unsqueeze(0)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            prob = torch.nn.functional.softmax(output)\n",
    "            label = classes[pred.item()]\n",
    "            conf = prob[0][pred.item()].item()\n",
    "            print(label, conf)\n",
    "    img_array.append(put_text(img, label, conf))\n",
    "\n",
    "\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
